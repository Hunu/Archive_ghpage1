---
layout: "post"
title: "Machine Learning Crash Course 04"
date: "2020-02-02 16:38"
---

# Machine Learning Crash Course 04 - 降低损失

1. TOC
{:toc}

在上一讲中，我们学习了如何计算损失，那么我们应该如何设定模型的参数，以使损失最小化呢？

我们可以将寻找更小损失的过程比做一个连续的过程，也就是说，当我们不断增加（或不断减小）某个（或某组）参数时，模型的损失将是持续变小的。

假如我们能够找出一个能够使损失持续变小的方向，那么将有助于建立良好的模型。

## 寻找方向（迭代方向）

寻找这个方向的其中一个方法就是计算斜率（即与模型参数相关的损失方程的导数）。

像均方差这样的简单损失函数的导数是很容易被计算的。借助其导数，我们可以非常有效的更新模型参数。

当找准了方向之后，我们只需要像那个方向一点一点的前进（调整参数），从而找到损失的最小值。前进的步伐越小，找到更小损失的几率就越大。这种方法叫梯度下降法（Gradient Descent）。

## 梯度下降法

我们可以将梯度下降法比做下面这种迭代方法：

![ML04-01](/images/2020/02/ml04-01.png)

我们通过计算代入模型的一组数据的损失函数的导数，我们通过导数的正负来判断我们应该朝哪个方向调整参数，从而降低损失。确定方向后，我们向那个方向前进一小步，从而获得了一组新的模型参数。然后再用新的模型重复上面的过程。

假如在一个纬度中，损失方程大致应是下面这个曲线：

![ML04-02](/images/2020/02/ml04-02.png)

上面这个曲线反映了模型参数θ与误差（损失）之间的关系。我们随机挑选一个θ值作为迭代的起点。同时可以看到对应的误差，如下图：

![ml04-03](/images/2020/02/ml04-03.png)

然后我们计算这一点的斜率，由斜率（为负）可知我们应该朝哪个方向前进（正）。当我们向那个方向前进一步后，我们就获得了一个新的模型参数及其对应的误差（损失）。

我们继续朝着这个方向梯度前进，直到我们得到了一个误差曲线的斜率为正的参数，这意味着我们应该向相反的方向去寻找。如下图：

![ml04-04](/images/2020/02/ml04-04.png)

## 学习速率

那么我们应该如何选择步进的幅度大小呢？这是由学习速率来决定的。

- 学习速率越高，到达最小损失所用的步数越**少**；
- 学习速率越低，到达最小损失所用的步数越**多**。

然而，

- 过高的学习速率可能导致模型冲出最低损失点，造成模型分叉；
- 过低的学习速率需要更多的时间。

为进一步理解**学习速率**的影响，我们可以尝试通过[这个](https://developers.google.com/machine-learning/crash-course/fitter/graph)模拟学习过程。

## 迭代起点

前面我们了解了什么是梯度下降法，并且了解到了我们从某一点作为起点，持续的朝着更小损失的方向寻找适当的模型参数。那么，我们是不是从任何一点作为起点都可以呢？

这个时候，然我们回想一下当年学过的微积分，或者一元二次方程 -- 他们的曲线是不是像一个巨大的碗？

![ml04-05](/images/2020/02/ml04-05.png)

所以，只要我们从这个大碗上的任意一点出发，都能根据斜率找出我们应前进的方向，然后选择学习速率，最终找到最小损失（误差）点。

然而，大多数机器学习问题都不是这么简单。众所周知，神经网络就不是这种“碗”形的，而更像是鸡蛋架。也就是说，神经网络可能会有很多的（阶段性）最小损失点。其中的一些最小损失点要优于其他的最小损失点。这种时候，选择一个好的起点就显得尤为重要了，这个我们后续再深入探讨。

![ml-04-06](/images/2020/02/ml-04-06.png)
![ml-04-06-cn](/images/2020/02/ml-04-06-cn.png)

## 提高迭代效率

我们先花一点时间来考虑一下效率的问题。

当我们计算误差（损失）方程的斜率的时候，从数学的角度来说我们应该计算所有样本的模型斜率，这是**确保**我们判断迭代方向正确的唯一方法。

然而，对于拥有上百万或者更多的数据集来说，这将会是一个巨大的计算工程。

根据经验，人们发现相比于用整个数据集进行迭代（建立模型），用数据集中的其中一个样本进行训练的效率会高很多。即使因此需要增加迭代次数，最终所需进行的整体计算量也会少很多。

这种随机抽取一个样本进行模型迭代的方法叫做**随机梯度下降法 Stochastic Gradient Descent**。

在实际应用中，我们衍生出了一种更折中的方案，那就是随机抽取一小部分样本，而不是只抽取一个，这一小部分样本通常由10-1000个样本组成，使用这一小部分样本来进行模型迭代。这种方法叫做**小批量梯度下降法 Mini-Batch Gradient Descent**。

## 降低损失 - 迭代方法

梯度下降法的第一步是选择一个$$w_1$$的起始点。起始点的选择并不重要，所以，许多算法都将起始$$w_1$$设置为0，或者选择一个更为随机的数字。下图中我们选择了一个略微大于0的值作为起始点

![figure3 a starting point for gradient descent](/images/2020/02/figure3-a-starting-point-for-gradient-descent.png)

随后，我们使用梯度下降法中的数学方法来计算该损失曲线在起始点的梯度。在上图中，起始点的损失的斜率等于曲线函数在起始点的导数。

  > 当存在多个权重时，梯度是相对于权重的偏导数的向量。

请注意，梯度是一个矢量，矢量具有（且仅具有）一下两个特征：

- 方向
- 大小

# todo continue on https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent?hl=en
